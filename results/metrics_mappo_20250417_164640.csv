epoch,reward,policy_loss,value_loss,entropy_loss,total_loss,computation_cost,transmission_cost,delay_penalty,training_time,agent_type,params,inference_time,estimated_cost,steps_per_episode,n_agents,agent_0_policy_loss,agent_0_value_loss,agent_0_entropy_loss
0,-9.527796843141934,-0.007096580229699612,7343.60791015625,0.6928659081459045,7342.907947667874,0.0,0.0,0.0,8.36489200592041,MAPPO,0,0,2.0833500000000003e-05,1000,1,-0.007096580229699612,7343.60791015625,0.6928659081459045
1,-0.7399561099733525,-0.004882301669567823,8376.130859375,0.6929519772529602,8375.433025096077,0.0,0.0,0.0,7.567239761352539,MAPPO,0,0,2.0833500000000003e-05,1000,1,-0.004882301669567823,8376.130859375,0.6929519772529602
2,-1.3609812662118816,-0.004151944536715746,8059.86376953125,0.6930215358734131,8059.16659605084,0.0,0.0,0.0,7.693959712982178,MAPPO,0,0,2.0833500000000003e-05,1000,1,-0.004151944536715746,8059.86376953125,0.6930215358734131
3,-8.185827106747015,-0.004250525962561369,8272.876953125,0.6930736899375916,8272.1796289091,0.0,0.0,0.0,8.014800071716309,MAPPO,0,0,2.0833500000000003e-05,1000,1,-0.004250525962561369,8272.876953125,0.6930736899375916
4,-9.147857823206126,-0.0030221848282963037,7453.189453125,0.693107008934021,7452.493323931238,0.0,0.0,0.0,7.65900731086731,MAPPO,0,0,2.0833500000000003e-05,1000,1,-0.0030221848282963037,7453.189453125,0.693107008934021
